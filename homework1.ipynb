{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine_learning_hw1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abeSanchez/Machine-Learning-UCF/blob/master/homework1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Rey980t7ipsI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Homework 1"
      ]
    },
    {
      "metadata": {
        "id": "GQSB4xJDjnjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c19ec7ed-8939-4aed-db5c-a31ea256a31a"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZfKZo3xkjpk6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "  sz = sigmoid(z)\n",
        "  return sz * (1 - sz)\n",
        "\n",
        "def se_loss(a, y):\n",
        "  return 0.5 * (a - y) ** 2\n",
        "\n",
        "def se_gradient(a, x, y):\n",
        "  return (a - y) * sigmoid_prime(a) * x\n",
        "\n",
        "def bce_loss(a, y):\n",
        "  return -y * np.log10(a + 1e-9) - (1 - y) * np.log10(1 - a + 1e-9)\n",
        "\n",
        "def bce_gradient(a, x, y):\n",
        "  return (a - y) * x\n",
        "\n",
        "def cce_loss(a, y):\n",
        "  return -np.sum(y * np.log10(a + 1e-9))\n",
        "\n",
        "def cce_gradient(a, x, y):\n",
        "  return (a - y) * x\n",
        "\n",
        "def softmax(z):\n",
        "  exps = np.exp(z - np.max(z))\n",
        "  return exps / np.sum(exps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mTjCuKfTS8rw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Flatten images and apply feature scaling\n",
        "flat_x_train = x_train.reshape(60000, 28 * 28) / 255\n",
        "flat_x_test = x_test.reshape(10000, 28 * 28) / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pTN2hv9DfGFo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create new array for labels for each digit\n",
        "y_train_single_digit = {}\n",
        "y_test_single_digit = {}\n",
        "\n",
        "for i in range(10):\n",
        "  y_train_single_digit[i] = y_train == i\n",
        "  y_train_single_digit[i] = y_train_single_digit[i].astype(int)\n",
        "  y_test_single_digit[i] = y_test == i\n",
        "  y_test_single_digit[i] = y_test_single_digit[i].astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jPICfylNcgU7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# One hot encoding for labels\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xqTuQWJnewgM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create dictionaries for storing the weights\n",
        "w_mse = {}\n",
        "w_bce = {}\n",
        "\n",
        "for i in range(10):\n",
        "  w_mse[i] = (np.random.rand(28 * 28, 1).reshape((784,)) - 0.5, np.random.rand(1))\n",
        "  w_bce[i] = (np.random.rand(28 * 28, 1).reshape((784,)) - 0.5, np.random.rand(1))\n",
        "  \n",
        "w_cce = np.random.rand(28 * 28 * 10, 1).reshape((784,10)) - 0.5 \n",
        "w_cce_b = np.random.rand(10, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BLoi3gHPd7l7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 1\n",
        "## Using logistic regression with mean squared error loss"
      ]
    },
    {
      "metadata": {
        "id": "7NlWsML03MZf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the classifiers"
      ]
    },
    {
      "metadata": {
        "id": "2SFn2628XuXb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "lr = 0.05\n",
        "batch_size = 32\n",
        "\n",
        "for digit in range(10):\n",
        "  for epoch in range(epochs):\n",
        "    shuffled_indices = np.random.permutation(len(flat_x_train))\n",
        "    flat_x_train_shuffled = flat_x_train[shuffled_indices]\n",
        "    y_train_single_digit_shuffled = y_train_single_digit[digit][shuffled_indices]\n",
        "    \n",
        "    for i in range(0, len(flat_x_train), batch_size):\n",
        "      xi = flat_x_train_shuffled[i:i+batch_size]\n",
        "      yi = y_train_single_digit_shuffled[i:i+batch_size].reshape(32,1)\n",
        "      z = xi.dot(w_mse[digit][0]) + w_mse[digit][1]\n",
        "      a = sigmoid(z).reshape(32,1)\n",
        "      gradient = np.sum(se_gradient(a, xi, yi), axis=0) / batch_size\n",
        "      gradient_b = np.sum(se_gradient(a, 1, yi), axis=0) / batch_size\n",
        "      w_mse[digit] = (w_mse[digit][0] - lr * gradient, w_mse[digit][1] - lr * gradient_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8JdkKd5B3RUX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing the classifiers"
      ]
    },
    {
      "metadata": {
        "id": "4E9KZXYSXXBk",
        "colab_type": "code",
        "outputId": "3189ec0c-6d15-42d3-bae4-a658d63c5aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "for digit in range(10):\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  for example in range(len(flat_x_test)):\n",
        "      z = flat_x_test[example].dot(w_mse[digit][0]) + w_mse[digit][1]\n",
        "      a = sigmoid(z)\n",
        "      prediction = int(round(a[0]))\n",
        "      loss += se_loss(prediction, y_test_single_digit[digit][example])\n",
        "      correct += 1 if y_test_single_digit[digit][example] == prediction else 0\n",
        "        \n",
        "  loss /= len(flat_x_test)\n",
        "  accuracy = correct / len(flat_x_test)\n",
        "  print(\"Classifier \" + str(digit) + \" | loss: \" + str(loss) + \" - accuracy: \" + str(accuracy))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier 0 | loss: 0.0041 - accuracy: 0.9918\n",
            "Classifier 1 | loss: 0.00375 - accuracy: 0.9925\n",
            "Classifier 2 | loss: 0.0106 - accuracy: 0.9788\n",
            "Classifier 3 | loss: 0.0117 - accuracy: 0.9766\n",
            "Classifier 4 | loss: 0.0103 - accuracy: 0.9794\n",
            "Classifier 5 | loss: 0.01405 - accuracy: 0.9719\n",
            "Classifier 6 | loss: 0.00765 - accuracy: 0.9847\n",
            "Classifier 7 | loss: 0.0079 - accuracy: 0.9842\n",
            "Classifier 8 | loss: 0.0208 - accuracy: 0.9584\n",
            "Classifier 9 | loss: 0.0184 - accuracy: 0.9632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n8KzgR67KxLZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using all classifiers to perform digit classification on test data"
      ]
    },
    {
      "metadata": {
        "id": "7vYfgNIFLIh8",
        "colab_type": "code",
        "outputId": "6f6a11ff-5630-4083-c413-d57dae282227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "\n",
        "for example in range(len(flat_x_test)):\n",
        "  y_predict = np.array([])  \n",
        "  \n",
        "  for classifier in range(10):\n",
        "    z = flat_x_test[example].dot(w_mse[classifier][0]) + w_mse[classifier][1]\n",
        "    a = sigmoid(z)\n",
        "    y_predict = np.append(y_predict, a)\n",
        "    \n",
        "  if np.argmax(y_predict) == y_test[example]:\n",
        "    correct += 1\n",
        "    \n",
        "print(\"Accuracy on test data : \" + str(correct / len(flat_x_test)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test data : 0.9109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w9vjKOevBzn7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 2\n",
        "## Using logistic regression with binary cross entropy loss"
      ]
    },
    {
      "metadata": {
        "id": "sgfo_Ou7CHFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the classifiers"
      ]
    },
    {
      "metadata": {
        "id": "ndVlQ-TPCAgb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "lr = 0.05\n",
        "batch_size = 32\n",
        "\n",
        "for digit in range(10):\n",
        "  for epoch in range(epochs):\n",
        "    shuffled_indices = np.random.permutation(len(flat_x_train))\n",
        "    flat_x_train_shuffled = flat_x_train[shuffled_indices]\n",
        "    y_train_single_digit_shuffled = y_train_single_digit[digit][shuffled_indices]\n",
        "    \n",
        "    for i in range(0, len(flat_x_train), batch_size):\n",
        "      xi = flat_x_train_shuffled[i:i+batch_size]\n",
        "      yi = y_train_single_digit_shuffled[i:i+batch_size].reshape(32,1)\n",
        "      z = xi.dot(w_bce[digit][0]) + w_bce[digit][1]\n",
        "      a = sigmoid(z).reshape(32,1)\n",
        "      gradient = np.sum(bce_gradient(a, xi, yi), axis=0) / batch_size\n",
        "      gradient_b = np.sum(bce_gradient(a, 1, yi), axis=0) / batch_size\n",
        "      w_bce[digit] = (w_bce[digit][0] - lr * gradient, w_bce[digit][1] - lr * gradient_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sZMtpc2xCKqf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing the classifiers"
      ]
    },
    {
      "metadata": {
        "id": "tKl2vlTMCM4x",
        "colab_type": "code",
        "outputId": "9388631b-592d-420c-9bb3-8274340d41a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "for digit in range(10):\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "  \n",
        "  for example in range(len(flat_x_test)):\n",
        "      z = flat_x_test[example].dot(w_bce[digit][0]) + w_bce[digit][1]\n",
        "      a = sigmoid(z)\n",
        "      prediction = int(round(a[0]))\n",
        "      loss += bce_loss(prediction, y_test_single_digit[digit][example])\n",
        "      correct += 1 if y_test_single_digit[digit][example] == prediction else 0\n",
        "        \n",
        "  loss /= len(flat_x_test)\n",
        "  accuracy = correct / len(flat_x_test)\n",
        "  print(\"Classifier \" + str(digit) + \" | loss: \" + str(loss) + \" - accuracy: \" + str(accuracy))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier 0 | loss: 0.07199999956918797 - accuracy: 0.992\n",
            "Classifier 1 | loss: 0.06119999956866659 - accuracy: 0.9932\n",
            "Classifier 2 | loss: 0.1808999995744447 - accuracy: 0.9799\n",
            "Classifier 3 | loss: 0.20609999957566064 - accuracy: 0.9771\n",
            "Classifier 4 | loss: 0.15389999957314154 - accuracy: 0.9829\n",
            "Classifier 5 | loss: 0.22409999957652907 - accuracy: 0.9751\n",
            "Classifier 6 | loss: 0.11969999957149098 - accuracy: 0.9867\n",
            "Classifier 7 | loss: 0.14039999957249027 - accuracy: 0.9844\n",
            "Classifier 8 | loss: 0.34649999958243555 - accuracy: 0.9615\n",
            "Classifier 9 | loss: 0.30419999958039445 - accuracy: 0.9662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-x4g1xP5SyFl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using all classifiers to perform digit classification on test data"
      ]
    },
    {
      "metadata": {
        "id": "UnYABWJRNcOA",
        "colab_type": "code",
        "outputId": "84c92d81-4aa7-454b-8ca4-44a8ab7a6764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "\n",
        "for example in range(len(flat_x_test)):\n",
        "  y_predict = np.array([])  \n",
        "  \n",
        "  for classifier in range(10):\n",
        "    z = flat_x_test[example].dot(w_bce[classifier][0]) + w_bce[classifier][1]\n",
        "    a = sigmoid(z)\n",
        "    y_predict = np.append(y_predict, a)\n",
        "    \n",
        "  if np.argmax(y_predict) == y_test[example]:\n",
        "    correct += 1\n",
        "    \n",
        "print(\"Accuracy on test data : \" + str(correct / len(flat_x_test)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test data : 0.9161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-MPpVabKIrw4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 3\n",
        "## Using softmax and categorical cross entropy loss"
      ]
    },
    {
      "metadata": {
        "id": "vkH4UbGbAjqb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the network"
      ]
    },
    {
      "metadata": {
        "id": "Hk9yjm08yoYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "ef9ad8a1-1cb9-4874-aa41-bc3d885f34eb"
      },
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "lr = 0.01\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  shuffled_indices = np.random.permutation(len(flat_x_train))\n",
        "  flat_x_train_shuffled = flat_x_train[shuffled_indices]\n",
        "  y_train_shuffled = y_train_cat[shuffled_indices]\n",
        "  correct = 0\n",
        "  loss = 0\n",
        "  \n",
        "  for i in range(0, len(flat_x_train), batch_size):\n",
        "    xi = flat_x_train_shuffled[i:i+batch_size]\n",
        "    yi = y_train_shuffled[i:i+batch_size]\n",
        "      \n",
        "    for j in range(batch_size):\n",
        "      z = xi[j].dot(w_cce).reshape(10,1) + w_cce_b.reshape(10,1)\n",
        "      a = softmax(z)\n",
        "      correct += 1 if np.argmax(a) == np.argmax(yi[j]) else 0\n",
        "      loss += cce_loss(a, yi[j].reshape(10,1))\n",
        "      gradient = cce_gradient(a, xi[j], yi[j].reshape(10,1)) if j == 0 else gradient + cce_gradient(a, xi[j], yi[j].reshape(10,1))\n",
        "      gradient_b = cce_gradient(a, 1, yi[j].reshape(10,1)) if j == 0 else gradient_b + cce_gradient(a, 1, yi[j].reshape(10,1))\n",
        "    \n",
        "    gradient = gradient / batch_size\n",
        "    gradient_b = gradient_b / batch_size\n",
        "\n",
        "    w_cce = w_cce - lr * gradient.T\n",
        "    w_cce_b = w_cce_b - lr * gradient_b\n",
        "    \n",
        "  loss /= len(flat_x_train)\n",
        "  accuracy = correct / len(flat_x_train)\n",
        "  print(\"After epoch \" + str(epoch + 1) + \" | loss: \" + str(loss) + \" - accuracy: \" + str(accuracy))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After epoch 1 | loss: 0.15518335550584886 - accuracy: 0.89695\n",
            "After epoch 2 | loss: 0.1523986044335064 - accuracy: 0.8988333333333334\n",
            "After epoch 3 | loss: 0.15000243973397903 - accuracy: 0.90055\n",
            "After epoch 4 | loss: 0.1478709656101087 - accuracy: 0.9025333333333333\n",
            "After epoch 5 | loss: 0.14606991348107756 - accuracy: 0.9030833333333333\n",
            "After epoch 6 | loss: 0.14436018951092847 - accuracy: 0.9048333333333334\n",
            "After epoch 7 | loss: 0.14279564525260527 - accuracy: 0.9062\n",
            "After epoch 8 | loss: 0.14144549538561363 - accuracy: 0.90705\n",
            "After epoch 9 | loss: 0.14018041823260582 - accuracy: 0.9083\n",
            "After epoch 10 | loss: 0.13900374648994618 - accuracy: 0.9091833333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3Gld87G37lsg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing the network on test data"
      ]
    },
    {
      "metadata": {
        "id": "ppnIzORwxrGJ",
        "colab_type": "code",
        "outputId": "d134da15-3af7-422c-8157-67afe8520d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "loss = 0\n",
        "correct = 0\n",
        "\n",
        "for example in range(len(flat_x_test)):\n",
        "  z = flat_x_test[example].dot(w_cce).reshape(10,1) + w_cce_b.reshape(10,1)\n",
        "  y_predict = softmax(z)\n",
        "  correct += 1 if np.argmax(y_predict) == y_test[example] else 0\n",
        "  loss += cce_loss(y_predict, y_test_cat[example].reshape(10,1))\n",
        "\n",
        "loss /= len(flat_x_test)\n",
        "accuracy = correct / len(flat_x_test)\n",
        "print(\"loss: \" + str(loss) + \" - accuracy: \" + str(accuracy))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 0.13595054791523534 - accuracy: 0.9101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JtnARSOKJQBU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 4\n",
        "## Reimplementing the network from Problem 3 entirely in Keras"
      ]
    },
    {
      "metadata": {
        "id": "P8WcVhM-AwS9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setting up the network architecture"
      ]
    },
    {
      "metadata": {
        "id": "rkGQoJokgnm4",
        "colab_type": "code",
        "outputId": "c835f107-7891-4320-f27a-5ddc421c28bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "cell_type": "code",
      "source": [
        "network = models.Sequential()\n",
        "network.add(layers.Dense(10, activation='softmax', input_shape=(28 * 28,)))\n",
        "network.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4CoIk0H1BCyo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compiling the network"
      ]
    },
    {
      "metadata": {
        "id": "7eKitTIzgxMS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "network.compile(optimizer='sgd',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xMCckksyBFh4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the network"
      ]
    },
    {
      "metadata": {
        "id": "BJkXOR22jn77",
        "colab_type": "code",
        "outputId": "e3157076-0b1f-4b2e-cbb1-ffcd6f3cb44d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "network.fit(flat_x_train, \n",
        "            y_train_cat, \n",
        "            epochs=epochs, \n",
        "            batch_size=32, \n",
        "            validation_data=(flat_x_test, y_test_cat))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 8s 141us/step - loss: 0.7832 - acc: 0.8115 - val_loss: 0.4832 - val_acc: 0.8807\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.4576 - acc: 0.8797 - val_loss: 0.4020 - val_acc: 0.8939\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.4043 - acc: 0.8908 - val_loss: 0.3694 - val_acc: 0.9003\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.3776 - acc: 0.8960 - val_loss: 0.3506 - val_acc: 0.9043\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.3607 - acc: 0.9006 - val_loss: 0.3372 - val_acc: 0.9090\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.3487 - acc: 0.9025 - val_loss: 0.3299 - val_acc: 0.9096\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.3398 - acc: 0.9056 - val_loss: 0.3217 - val_acc: 0.9124\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.3326 - acc: 0.9074 - val_loss: 0.3160 - val_acc: 0.9125\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.3267 - acc: 0.9088 - val_loss: 0.3110 - val_acc: 0.9154\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.3217 - acc: 0.9101 - val_loss: 0.3074 - val_acc: 0.9152\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc8015609e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "vauExO-liknz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 5\n",
        "## Extending the network from Problem 4 by adding new features"
      ]
    },
    {
      "metadata": {
        "id": "1N_QQrANwTE0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Methods for extracting features"
      ]
    },
    {
      "metadata": {
        "id": "10V6ru699NUo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def round_grey_values(original_image):\n",
        "  return np.round(original_image / 255).astype(int)\n",
        "\n",
        "# Get number of white regions for an image (Threshold = 100)\n",
        "def num_white_regions(original_image):\n",
        "  image = np.copy(original_image)\n",
        "  num_regions = 0\n",
        "  \n",
        "  for i in range(len(image)):\n",
        "    for j in range(len(image[i])):\n",
        "      if image[i,j] == 1:\n",
        "        num_regions += 1\n",
        "        visit_region(image, i, j)\n",
        "        \n",
        "  return num_regions\n",
        "\n",
        "# num_white_regions() helper function\n",
        "def visit_region(image, i, j):\n",
        "  if (i < 0 or j < 0 or i >= len(image)\n",
        "      or j >= len(image[0]) or image[i,j] == 0):\n",
        "    return\n",
        "  \n",
        "  image[i,j] = 0\n",
        "  visit_region(image, i + 1, j)\n",
        "  visit_region(image, i - 1, j)\n",
        "  visit_region(image, i, j + 1)\n",
        "  visit_region(image, i, j - 1)\n",
        "\n",
        "def digit_width(image):\n",
        "  min_x = len(image)\n",
        "  max_x = 0\n",
        "  \n",
        "  for i in range(len(image)):\n",
        "    min_x = min(np.argmax(image[i]), min_x)\n",
        "    max_x = max(np.argmax(np.flip(image[i], 0)), max_x)\n",
        "  \n",
        "  return max_x - min_x\n",
        "\n",
        "def digit_height(image):\n",
        "  return digit_width(image.T)\n",
        "\n",
        "def digit_area(image):\n",
        "  return np.sum(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "R-EStuwyRVpq"
      },
      "cell_type": "markdown",
      "source": [
        "### Extract number of white regions from image"
      ]
    },
    {
      "metadata": {
        "id": "Et5hS1awqOXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_regions_train = []\n",
        "num_regions_test = []\n",
        "\n",
        "for image in range(len(x_train)):\n",
        "  num_regions_train.append(num_white_regions(x_train[image]))\n",
        "  \n",
        "for image in range(len(x_test)):\n",
        "  num_regions_test.append(num_white_regions(x_test[image]))\n",
        "  \n",
        "num_regions_train = np.array(num_regions_train)\n",
        "num_regions_test = np.array(num_regions_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bCR97A1hQBHq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Extract white regions, digit height, width and area from image"
      ]
    },
    {
      "metadata": {
        "id": "m-lfv7lYP-s5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "digit_width_train = []\n",
        "digit_width_test = []\n",
        "digit_height_train = []\n",
        "digit_height_test = []\n",
        "digit_area_train = []\n",
        "digit_area_test = []\n",
        "\n",
        "for image in range(len(x_train)):\n",
        "  digit_width_train.append(digit_width(x_train[image]))\n",
        "  digit_height_train.append(digit_height(x_train[image]))\n",
        "  digit_area_train.append(digit_area(x_train[image]))\n",
        "  \n",
        "for image in range(len(x_test)):\n",
        "  digit_width_test.append(digit_width(x_test[image]))\n",
        "  digit_height_test.append(digit_height(x_test[image]))\n",
        "  digit_area_test.append(digit_area(x_test[image]))\n",
        "  \n",
        "digit_width_train = np.array(digit_width_train)\n",
        "digit_width_test = np.array(digit_width_test)\n",
        "\n",
        "digit_height_train = np.array(digit_height_train)\n",
        "digit_height_test = np.array(digit_height_test)\n",
        "\n",
        "digit_area_train = np.array(digit_area_train)\n",
        "digit_area_test = np.array(digit_area_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3OtU_glglrLZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feature scale engineered data"
      ]
    },
    {
      "metadata": {
        "id": "KJmc6C_aR-D8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Find max and mins of train and test\n",
        "num_regions_min = min(np.min(num_regions_train), np.min(num_regions_test))\n",
        "num_regions_max = max(np.max(num_regions_train), np.max(num_regions_test))\n",
        "\n",
        "digit_width_min = min(np.min(digit_width_train), np.min(digit_width_test))\n",
        "digit_width_max = max(np.max(digit_width_train), np.max(digit_width_test))\n",
        "\n",
        "digit_height_min = min(np.min(digit_height_train), np.min(digit_height_test))\n",
        "digit_height_max = max(np.max(digit_height_train), np.max(digit_height_test))\n",
        "\n",
        "digit_area_min = min(np.min(digit_area_train), np.min(digit_area_test))\n",
        "digit_area_max = max(np.max(digit_area_train), np.max(digit_area_test))\n",
        "\n",
        "\n",
        "# Use found max and mins to feature scale\n",
        "num_regions_train = num_regions_train - num_regions_min\n",
        "num_regions_train = num_regions_train / num_regions_max\n",
        "\n",
        "num_regions_test = num_regions_test - num_regions_min\n",
        "num_regions_test = num_regions_test / num_regions_max\n",
        "\n",
        "digit_width_train = digit_width_train - digit_width_min\n",
        "digit_width_train = digit_width_train / digit_width_max\n",
        "\n",
        "digit_width_test = digit_width_test - digit_height_min\n",
        "digit_width_test = digit_width_test / digit_height_max\n",
        "\n",
        "digit_height_train = digit_height_train - digit_height_min\n",
        "digit_height_train = digit_height_train / digit_height_max\n",
        "\n",
        "digit_height_test = digit_height_test - digit_height_min\n",
        "digit_height_test = digit_height_test / digit_height_max\n",
        "\n",
        "digit_area_train = digit_area_train - digit_area_min\n",
        "digit_area_train = digit_area_train / digit_area_max\n",
        "\n",
        "digit_area_test = digit_area_test - digit_area_min\n",
        "digit_area_test = digit_area_test / digit_area_max"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uS7m2hkKvLLp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stack flattened image on top of engineered features"
      ]
    },
    {
      "metadata": {
        "id": "RhI4_Qh-Wbgw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "engineered_features_train = np.array([num_regions_train, \n",
        "                                      digit_width_train, \n",
        "                                      digit_height_train, \n",
        "                                      digit_area_train])\n",
        "engineered_features_test = np.array([num_regions_test, \n",
        "                                     digit_width_test, \n",
        "                                     digit_height_test, \n",
        "                                     digit_area_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TzfATotupJAO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "flat_x_train2 = []\n",
        "flat_x_test2 = []\n",
        "\n",
        "for i in range(len(flat_x_train)):\n",
        "  flat_x_train2.append(np.append(flat_x_train[i], engineered_features_train[:,i]))\n",
        "  \n",
        "for i in range(len(flat_x_test)):\n",
        "  flat_x_test2.append(np.append(flat_x_test[i], engineered_features_test[:,i]))\n",
        "  \n",
        "flat_x_train2 = np.array(flat_x_train2)\n",
        "flat_x_test2 = np.array(flat_x_test2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZpPbWgEWvdX5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setting up the network architecture"
      ]
    },
    {
      "metadata": {
        "id": "e3MRAzTDlFYH",
        "colab_type": "code",
        "outputId": "36d57690-bf45-4251-9e8a-9ca4da898fcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "network2 = models.Sequential()\n",
        "network2.add(layers.Dense(10, activation='softmax', input_shape=(28 * 28 + 4,)))\n",
        "network2.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 10)                7890      \n",
            "=================================================================\n",
            "Total params: 7,890\n",
            "Trainable params: 7,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rlCkbagbvorT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compiling the network"
      ]
    },
    {
      "metadata": {
        "id": "DAGkUYH4lLUR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "network2.compile(optimizer='sgd',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wxpwygY1vrzw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the network"
      ]
    },
    {
      "metadata": {
        "id": "k8w00hYTlPcr",
        "colab_type": "code",
        "outputId": "46067be7-6b60-4fd6-ceec-9e204be9d4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "network2.fit(flat_x_train2, \n",
        "            y_train_cat, \n",
        "            epochs=epochs, \n",
        "            batch_size=32, \n",
        "            validation_data=(flat_x_test2, y_test_cat))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.7686 - acc: 0.8176 - val_loss: 0.4788 - val_acc: 0.8808\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.4550 - acc: 0.8815 - val_loss: 0.3984 - val_acc: 0.8975\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.4022 - acc: 0.8918 - val_loss: 0.3658 - val_acc: 0.9035\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.3756 - acc: 0.8976 - val_loss: 0.3466 - val_acc: 0.9068\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.3588 - acc: 0.9011 - val_loss: 0.3353 - val_acc: 0.9095\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.3470 - acc: 0.9038 - val_loss: 0.3252 - val_acc: 0.9119\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.3379 - acc: 0.9060 - val_loss: 0.3185 - val_acc: 0.9123\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.3308 - acc: 0.9078 - val_loss: 0.3129 - val_acc: 0.9154\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.3249 - acc: 0.9095 - val_loss: 0.3089 - val_acc: 0.9164\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.3201 - acc: 0.9106 - val_loss: 0.3054 - val_acc: 0.9159\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc8013a6470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}